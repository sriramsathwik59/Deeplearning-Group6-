import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import os

# Define the Generator
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1),
            nn.Tanh()  # Output range [-1, 1]
        )

    def forward(self, x):
        return self.main(x)

# Define the Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),
            nn.AdaptiveAvgPool2d(1),  # Reduces each feature map to 1x1
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.main(x)


# Dataset Class
class FrameDataset(Dataset):
    def __init__(self, root_dir):
        self.frame_files = []
        for subdir in sorted(os.listdir(root_dir)):
            subdir_path = os.path.join(root_dir, subdir)
            if os.path.isdir(subdir_path):
                images = sorted([os.path.join(subdir_path, img) for img in os.listdir(subdir_path) if img.endswith('.jpg')])
                self.frame_files.extend(images)

    def __len__(self):
        return len(self.frame_files) - 1  # since we need a pair (current and next frame)

    def __getitem__(self, idx):
        frame = Image.open(self.frame_files[idx])
        next_frame = Image.open(self.frame_files[idx + 1])

        transform = transforms.Compose([
            transforms.ToTensor(),  # Converts to range [0, 1]
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]
        ])

        frame = transform(frame)
        next_frame = transform(next_frame)
        return frame, next_frame

# Instantiate models
generator = Generator()
discriminator = Discriminator()

# Optimizers
g_optimizer = optim.Adam(generator.parameters(), lr=0.002)
d_optimizer = optim.Adam(discriminator.parameters(), lr=0.002)

# Loss
criterion = nn.BCELoss()

# Training Loop

def train(data_loader, generator, discriminator, g_optimizer, d_optimizer, criterion, epochs=20):
    for epoch in range(epochs):
        for i, data in enumerate(data_loader):
            current_frames, next_frames = data

            real_labels = torch.ones(current_frames.size(0), 1, 1, 1)
            fake_labels = torch.zeros(current_frames.size(0), 1, 1, 1)

            discriminator.zero_grad()

            # Real frames
            real_outputs = discriminator(next_frames)
            real_outputs = torch.sigmoid(torch.mean(real_outputs, dim=[2, 3], keepdim=True))  # Apply global average pooling
            real_loss = criterion(real_outputs, real_labels)

            # Fake frames generated by the Generator
            fake_frames = generator(current_frames)
            fake_outputs = discriminator(fake_frames.detach())
            fake_outputs = torch.sigmoid(torch.mean(fake_outputs, dim=[2, 3], keepdim=True))  # Apply global average pooling
            fake_loss = criterion(fake_outputs, fake_labels)

            # Calculate total Discriminator loss
            d_loss = real_loss + fake_loss
            d_loss.backward()
            d_optimizer.step()

            # Generator update
            generator.zero_grad()
            outputs = discriminator(fake_frames)
            outputs = torch.sigmoid(torch.mean(outputs, dim=[2, 3], keepdim=True))  # Apply global average pooling
            g_loss = criterion(outputs, real_labels)
            g_loss.backward()
            g_optimizer.step()

            if i % 100 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Step [{i}/{len(data_loader)}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}')

# Main
if __name__ == "__main__":
    dataset = FrameDataset(root_dir='/home/ubuntu/Train/frames')
    loader = DataLoader(dataset, batch_size=10, shuffle=True)

    train(loader, generator, discriminator, g_optimizer, d_optimizer, criterion)
